# Tasks

## Devoxx Paris

- [x] generate the template
- [x] check the ports for ingestion/chat services
- [x] test ingestion
- [x] generate solution package
- [x] simplify ingestion: just run the script and explain parts of the code
- [x] use env vars for Qdrant host / collection name!
- [x] rename collection to use env variable / kbindex by default
- [x] check/fix citations format
- [x] 6334 for Qdrant in infra
- [x] update ingest-data scripts for java
- [x] update slides (new version?)

- [ ] .env at root project
- [ ] SA: dockerfiles in /backend and /ingestion

- [ ] generate aka.ms link: aka.ms/ws/openai-rag-quarkus
  + link w/ proxy in the slides
  + azpasses?

- [ ] SA: build&copy the frontend in java build
- [ ] feature: missing message history in chat: use input message and feed it to LC4J Memory with a token window, see https://github.com/langchain4j/langchain4j-examples/blob/main/other-examples/src/main/java/ChatMemoryExamples.java

### Maybe

- [ ] in improvements section: add follow-up questions
- [ ] in improvements section: add streaming

## For later

- [x] /!\ Update Azure OpenAI version

- [ ] rename ingestion to ingestion-node
- [ ] rework ingestion-node to use langchainjs
- [ ] use azure-openai sdk instead of openai

- [ ] Add troubleshooting for npm run dev backend if it fails (restart) timeout (node-qdrant)
  * try to reproduce!

- [ ] add notes if using OPENAI_TOKEN instead of AZURE OAI + KEY
- [ ] add docker and troubleshooting section (full rebuild container) / codespaces
- [ ] add npm i troubleshooting section (npm install) / codespaces

- [ ] improve frontend client
  * [ ] better catch errors when API is not available
  * [ ] update to latest version
  * [ ] fix fonts
  * [ ] for fun: add voice recognition using Web Speech Recognition API
  * [ ] clean up code

- [ ] improve workshop
  * [ ] add section in improvements about tools usage with LangChain.js and Retrievers
  * [ ] allow to use regular OpenAI API/token + Ollama

- [ ] trainer infra
  * [ ] add event key to restrict access?
  * [ ] get started docs on how to setup the workshop and the proxy
  * [ ] allow to configure how many OpenAI deployments you want, with region and capacity

